* [AutoScale Design Proposal](#autoscale-design-proposal)
   * [Motivation](#motivation)
   * [Goal](#goal)
   * [NonGoal](#nongoal)
   * [Design](#design)
      * [1.预先的工作：](#1-预先的工作)
      * [2.创建应用并查看 scale](#2创建应用并查看-scale)
      * [3.指定指标](#3指定指标)
      * [4.部署 HPA](#4部署-hpa)
      * [5.查看 HPA](#5查看-hpa)
      * [6.目前存在的问题](#6-目前存在的问题)

Created by [gh-md-toc](https://github.com/ekalinin/github-markdown-toc)

# AutoScale Design Proposal

## Motivation

用户在使用 Fluid 可能会遇到什么问题：

1. 影响任务运行
    1. 预留资源无法满足 train job 
        1. 可能存储和需要缓存数据的对等关系用户不可知，比如训练数据大小是 10G，但是缓存 Capacity 为 8G
        2. 训练过程中（可中断）新添加数据，而并不想清理缓存，但是预设资源不满足，比如原训练数据为 5G， 缓存 Capacity 为10G，新加训练数据10G，但是又不想重新缓存之前的 5G 数据
    2. 提供缓存节点出现故障，或者抖动
    3. 竞价实例 Spot 中断回收
2. 资源浪费
    1. 预留资源远超训练所需资源

## Goal

1. 在预留资源无法满足任务时，可以 ScaleUp 进行缓存拓展
2. 在预留缓存资源远超训练所需缓存资源，可以 ScaleIn 进行缓存收缩

## NonGoal

1. 节点故障容错对缓存的影响
2. 竞价实例回收对缓存的影响
3. 设置水位控制 scale up 速率， 比如 0.8 和 0.95 速率是不同的，参考 [https://github.com/DataDog/watermarkpodautoscaler](https://github.com/DataDog/watermarkpodautoscaler)

## Design

### 1.预先的工作：

1. 部署 metrics-server [https://github.com/kubernetes-sigs/metrics-server/](https://github.com/kubernetes-sigs/metrics-server/)，非必须
2. 最好 k8s 版本在1.18 以上，开启 HPA v2beta2 功能，我在 v1.18.8 版本测试
3. 根据 `fluid/tools/monitoring/prometheus.yaml` 部署 prometheus 服务, configmap 配置如下

    ```yaml
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: prometheus-configmap
      namespace: kube-system
    data:
      prometheus.yml: |-
        rule_files:
          - "/etc/prometheus-rules/*.rules"
        scrape_configs:
        - job_name: 'alluxio master'
          metrics_path: /metrics/prometheus
          kubernetes_sd_configs:
            - role: endpoints
          relabel_configs:
          - source_labels: [__meta_kubernetes_service_label_monitor]
            regex: alluxio_runtime_metrics
            action: keep
          - source_labels: [__meta_kubernetes_endpoint_port_name]
            regex: web
            action: keep
          - source_labels: [__meta_kubernetes_namespace]
            target_label: namespace
            replacement: $1
            action: replace
          - source_labels: [__meta_kubernetes_service_label_release]
            target_label: fluid_runtime
            replacement: $1
            action: replace
          - source_labels: [__meta_kubernetes_endpoint_address_target_name]
            target_label: pod
            replacement: $1
            action: replace
    ```

4. 部署 prometheus-hpa-adapter [https://github.com/stefanprodan/k8s-prom-hpa](https://github.com/stefanprodan/k8s-prom-hpa) 

    ```yaml
    # 修改 Makefile
    .PHONY: gencerts
    gencerts:
            @echo Generating TLS certs
            @docker pull cfssl/cfssl
            @mkdir -p output
            @touch output/apiserver.pem
            @touch output/apiserver-key.pem
            @openssl req -x509 -sha256 -new -nodes -days 365 -newkey rsa:2048 -keyout $(PURPOSE)-ca.key -out $(PURPOSE)-ca.crt -subj "/CN=ca"
            @echo '{"signing":{"default":{"expiry":"43800h","usages":["signing","key encipherment","'$(PURPOSE)'"]}}}' > "$(PURPOSE)-ca-config.json"
            @echo '{"CN":"'$(SERVICE_NAME)'","hosts":[$(ALT_NAMES)],"key":{"algo":"rsa","size":2048}}' | docker run  -v ${HOME}:${HOME} -v ${PWD}/metrics-ca.key:/go/src/github.com/cloudflare/cfssl/metrics-ca.key -v ${PWD}/metrics-ca.crt:/go/src/github.com/cloudflare/cfssl/metrics-ca.crt -v ${PWD}/metrics-ca-config.json:/go/src/github.com/cloudflare/cfssl/metrics-ca-config.json -i cfssl/cfssl gencert -ca='/go/src/github.com/cloudflare/cfssl/metrics-ca.crt' -ca-key='/go/src/github.com/cloudflare/cfssl/metrics-ca.key' -config='/go/src/github.com/cloudflare/cfssl/metrics-ca-config.json' - | docker run --entrypoint=cfssljson -v ${HOME}:${HOME} -v ${PWD}/output:/go/src/github.com/cloudflare/cfssl/output -i cfssl/cfssl -bare /go/src/github.com/cloudflare/cfssl/output/apiserver
    # 执行
    make certs
    # 进去 custom-metrics-api/
    # 修改 custom-metrics-config-map.yaml 
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: adapter-config
      namespace: monitoring
    data:
      config.yaml: |
        rules:
    		- seriesQuery: '{__name__=~"Cluster_(CapacityTotal|CapacityUsed)",fluid_runtime!="",instance!="",pod!="",job="alluxio master",namespace!=""}'
    		      seriesFilters:
    		      - is: ^Cluster_(CapacityTotal|CapacityUsed)$
    		      resources:
    		        overrides:
    		          namespace:
    		            resource: namespace
    		          pod:
    		            resource: pod
    		      name:
    		        matches: "^(.*)"
    		        as: "capacity_used_rate"
    		      metricsQuery: ceil(Cluster_CapacityUsed{<<.LabelMatchers>>}*100/(Cluster_CapacityTotal{<<.LabelMatchers>>}))
    # 修改custom-metrics-apiserver-deployment.yaml - --prometheus-url=http://prometheus-svc.kube-system.svc:9090/ 注意这里的服务名和 namespace 和上面部署的 prometheus 一致
    # kubectl apply -f ./
    ```

5. 修改代码

```go
// For api/v1alpha1/alluxioruntime_types.go
// +kubebuilder:subresource:scale:specpath=.spec.replicas,statuspath=.status.desiredWorkerNumberScheduled,selectorpath=.status.selector
type AlluxioRuntime struct {
}

type RuntimeSpec struct {
	Replicas int32    `json:"replicas"`
}

// For api/v1alpha1/status.go
type RuntimeStatus struct {
		DesiredWorkerNumberScheduled int32   `json:"desiredWorkerNumberScheduled,omitempty"`
    Selector string  `json:"selector,omitempty"`
}

// For pkg/ddc/alluxio/status.go
selectorStr := "name" + "=" + masterName
// //selectorStr := "app" + "=" + workerName + "-" + namespace
//selector, err := labels.Parse(selectorStr)
//if err != nil {
//      return err
//}
runtimeToUpdate.Status.Selector = selectorStr
```

```bash
export ALLUXIORUNTIME_CONTROLLER_IMG=xieydd/fluid-runtime-controller  && make docker-build-alluxioruntime-controller
# After push 
helm upgrade fluid -f charts/fluid/fluid/values.yaml --set runtime.alluxio.controller.image=xieydd/fluid-runtime-controller:v0.5.0-f625d01 charts/fluid/fluid/
```

### 2.创建应用并查看 scale

1. 创建应用
```yaml
apiVersion: data.fluid.io/v1alpha1
kind: Dataset
metadata:
  name: hbase
spec:
  mounts:
    - mountPoint: https://mirrors.tuna.tsinghua.edu.cn/apache/hbase/stable/
      name: hbase

apiVersion: data.fluid.io/v1alpha1
kind: AlluxioRuntime
metadata:
  name: hbase
spec:
  replicas: 1
  tieredstore:
    levels:
      - mediumtype: MEM
        path: /dev/shm
        quota: 2Gi
        high: "0.95"
        low: "0.7"
  properties:
    alluxio.user.block.size.bytes.default: 256MB
    alluxio.user.streaming.reader.chunk.size.bytes: 256MB
    alluxio.user.local.reader.chunk.size.bytes: 256MB
    alluxio.worker.network.reader.buffer.size: 256MB
    alluxio.user.streaming.data.timeout: 300sec
  fuse:
    args:
      - fuse
      - --fuse-opts=kernel_cache,ro,max_read=131072,attr_timeout=7200,entry_timeout=7200,nonempty,max_readahead=0
  monitoring: true
```
2. 查看 scale 
```bash
// update crd and deploy hbase demo and check
$ kubectl get --raw /apis/data.fluid.io/v1alpha1/namespaces/default/alluxioruntimes/hbase/scale | jq
{
  "kind": "Scale",
  "apiVersion": "autoscaling/v1",
  "metadata": {
    "name": "hbase",
    "namespace": "default",
    "selfLink": "/apis/data.fluid.io/v1alpha1/namespaces/default/alluxioruntimes/hbase/scale",
    "uid": "174cd0c2-06c8-4ee5-9b1b-de2a986cfe4d",
    "resourceVersion": "81156219",
    "creationTimestamp": "2021-01-11T07:13:00Z"
  },
  "spec": {
    "replicas": 1
  },
  "status": {
    "replicas": 1,
		"selector": "name=hbase-master" # set for metrics pod label, for this is alluxip master pod
  }
}
```

### 3.指定指标

任务相关：

1. 缓存使用百分率：Cluster_CapacityUsed*100 / Cluster_CapacityTotal

2. 查看指标

```bash
# kubectl get --raw "/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/*/capacity_used_rate?labelSelector=app%3Dalluxio" | jq
{
  "kind": "MetricValueList",
  "apiVersion": "custom.metrics.k8s.io/v1beta1",
  "metadata": {
    "selfLink": "/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/%2A/capacity_used_rate"
  },
  "items": [
    {
      "describedObject": {
        "kind": "Pod",
        "namespace": "default",
        "name": "hbase-master-0",
        "apiVersion": "/v1"
      },
      "metricName": "capacity_used_rate",
      "timestamp": "2021-02-08T13:40:30Z",
      "value": "0"
    }
  ]
}
```

### 4.部署 HPA

```yaml
apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
  name: alluxio-custom-hpa
spec:
  scaleTargetRef:
    apiVersion: data.fluid.io/v1alpha1
    kind: AlluxioRuntime
    name: hbase
  minReplicas: 1
  maxReplicas: 3
  metrics:
  - type: Pods
    pods:
      metric:
        name: capacity_used_rate
      target:
       type: Value
       averageValue: "15" # for test 15% 
```

### 5.查看 HPA

```bash
# kubectl  describe hpa alluxio-custom-hpa
Name:                            alluxio-custom-hpa
Namespace:                       default
Labels:                          <none>
Annotations:                     CreationTimestamp:  Thu, 21 Jan 2021 10:42:30 +0800
Reference:                       AlluxioRuntime/hbase
Metrics:                         ( current / target )
  "capacity_used_rate" on pods:  14 / 15
Min replicas:                    1
Max replicas:                    3
AlluxioRuntime pods:             2 current / 2 desired
Conditions:
  Type            Status  Reason              Message
  ----            ------  ------              -------
  AbleToScale     True    ReadyForNewScale    recommended size matches current size
  ScalingActive   True    ValidMetricFound    the HPA was able to successfully calculate a replica count from pods metric capacity_used_rate
  ScalingLimited  False   DesiredWithinRange  the desired count is within the acceptable range
Events:
  Type    Reason             Age    From                       Message
  ----    ------             ----   ----                       -------
  Normal  SuccessfulRescale  5m23s  horizontal-pod-autoscaler  New size: 2; reason: pods metric capacity_used_rate above target
# kubectl get alluxioruntime -o yaml | grep replicasetricsQuery: ceil(Cluster_CapacityUsed{<<.LabelMatchers>>}*100/(Cluster_CapacityTotal{<<.LabelMatchers>>}))

# kubectl apply -f app.yaml
# cat app.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: fluid-copy-test-hbase
spec:
  template:
    spec:
      restartPolicy: OnFailure
      containers:
        - name: busybox
          image: busybox
          command: ["/bin/sh"]
          args: ["-c", "set -x; time cp -r /data/hbase ./"]
          volumeMounts:
            - mountPath: /data
              name: hbase-vol
      volumes:
        - name: hbase-vol
          persistentVolumeClaim:
            claimName: hbase
      #nodeName: cn-beijing.192.168.0.198

# kubectl get hpa --watch
NAME                 REFERENCE              TARGETS        MINPODS   MAXPODS   REPLICAS   AGE
alluxio-custom-hpa   AlluxioRuntime/hbase   <unknown>/15   1         3         0          3s
alluxio-custom-hpa   AlluxioRuntime/hbase   0/15           1         3         1          16s
alluxio-custom-hpa   AlluxioRuntime/hbase   17/15          1         3         1          92s
alluxio-custom-hpa   AlluxioRuntime/hbase   17/15          1         3         2          107s
alluxio-custom-hpa   AlluxioRuntime/hbase   14/15          1         3         2          2m33s
alluxio-custom-hpa   AlluxioRuntime/hbase   18/15          1         3         2          3m35s
alluxio-custom-hpa   AlluxioRuntime/hbase   14/15          1         3         2          4m36s

# kubectl  exec -it hbase-master-0 /bin/bash
alluxio fs free  -f /

alluxio-custom-hpa   AlluxioRuntime/hbase   0/15           1         3         2          23m
alluxio-custom-hpa   AlluxioRuntime/hbase   0/15           1         3         2          28m
alluxio-custom-hpa   AlluxioRuntime/hbase   0/15           1         3         1          28m
```

### 6.目前存在的问题

1. scaledown 5min 很慢
2. Alluxioruntime replicas spec from 2 to 1 but worker not scale down, [https://github.com/fluid-cloudnative/fluid/issues/545](https://github.com/fluid-cloudnative/fluid/issues/545)
