# DataFlow中的Data Operation Pod 间的亲和性

## 动机
当前Fluid 支持 DataFlow，多个Data Operation可以顺序执行。但是不同的 Data Operation 的 Pod 在执行时，可能会在不同的节点，导致整体性能不高：

- 如果后面的 Data Operation 能够在前面的 Data Operation 同样的 node/zone/region 上执行，可能会使得性能更好；
## 目标

- 支持 DataFlow 中不同的 Data Operations的 Pod 的亲和性"继承"， 调度到同一个 node/zone/region，提升整体的 DataFlow 的性能；
- 支持不同的亲和性"继承"策略，默认“”（空值），保持现状不变，可选 Require / Prefer；
- CRD 支持设置 Require/Prefer 的 node label，用于亲和性匹配，默认使用 `kubernetes.io/hostname` 的label；
## 设计

### 0. 流程图

对于 Data Operation 的 Reconcile 逻辑，在 Executing 阶段，主要新增以下流程：

- Data Operation 在 Status 新增 Pod 运行时的节点信息；
- Data Operation Pod 的亲和性，**根据 AffinityStrategy 添加从上一个 Data Operation Status 中匹配的亲和性**；

![img](images/data_flow_affinity_flow.jpeg)

### 1. Data Operation Status 新增运行时的节点信息

Data Operation 运行的时候，需要在 OperationStatus 中添加运行的节点信息（如节点HostName，节点所在的 Region/Zone等）。

```go
type OperationStatus struct {
    // Added Field.
    // stores the operation pod's node info, key is a host label.
    // +optional
    NodeAffinityLabels map[string][string]
}
```

1. Fluid 会根据 Operation Pod 所在的节点，添加`kubernetes.io/hostname`、`topology.kubernetes.io/zone`、`topology.kubernetes.io/region`的标签信息；

2. 如果用户在 Operation Spec 中定义了 NodeAffinity，Fluid 也会根据 key 去 Pod 所在的 Node的 labels 去匹配并记录键值对，如果不存在则不记录；
   - 无论用户定义的 NodeSelectorOperator 是`In/NotIn/Exists/Lt/..`，后面都换转为成 `In`类型的 NodeAffinity 被后续的Op使用；

3. <font color='red'>如果 Operation 是个分布式作业，具备多个Pod，则不记录运行时的节点信息；</font>

### 2. Data Operation 根据前一个Op的运行时信息添加亲和性调度

DataOperation 的定义中，需要添加相应字段：
```go
// the referent operation
type OperationRef struct {
    // Added Field.
    // Namespace specifies the pod affinity strategy with the referent operation.
    // +optional
    AffinityStrategy *AffinityStrategy `json:"affinityStrategy,omitempty"`
}

// The following are all newly added codes.
type AffinityPolicy string

const (
	DefaultAffinityPolicy AffinityPolicy = ""
	RequireAffinityPolicy AffinityPolicy = "Require"
	PreferAffinityPolicy  AffinityPolicy = "Prefer"
)

type AffinityStrategy struct {
    // Policy one of: "", "Require", "Prefer"
    // +optional 
    Policy AffinityPolicy `json:policy,omitemtpy"`

    // Require define the matching label names when Policy is 'require'.
    // use "kubernetes.io/hostname" laby default.
    // +optional 
    Require []string `json:"require,omitempty"`
    
    // Require define the matching label names and weights when Policy is 'prefer'.
    // use "kubernetes.io/hostname" name and weight 100 by default.
    // +optional 
    Prefer  []Prefer `json:"prefer,omitempty"`
}

type Prefer struct {
    Name   string `json:"name"`
    Weight int    `json:"weight"`
}
```

### 亲和性匹配继承逻辑
Data Operation 在生成 Helm Values 文件时，需要**根据上一个 DataOperation Status 中 Node 位置信息，设置自己 Pod 的 Affinity字段**：

- 每个 Data Operation 的每个实现引擎，都要新增该逻辑；

**每个 Operation 执行时的 PodAffinity 的计算公式**如下：

- $PodAffinity(Op) = Op.Spec.Affinity + MatchAndGenerateAffinity(Op)$


```go
func MatchAndGenerateAffinity(current DataOperation) *corev1.Affinity {
    // 如果 `Op` 没有前置操作，则 MatchedAffinity 返回 `nil`
    if current.Spec.RunAfter == nil {
        return nil
    }
    // 获取前置操作的运行时的节点信息
    prevOpNodeLabels := getReferentOp(current.Spec.RunAfter).Status.NodeAffinityLabels
    
    policy := current.Spec.RunAfter.AffinityStratgey.Policy

    // 1. 默认不配置，从上个操作中不继承任何亲和性
    if policy == "" {
        return nil
    }
    
    // 上一个Op Status 中没有节点位置信息
    if prevOpNodeLabels == nil {
        return nil
    }
    
    // 2. require 亲和性继承
    if policy == "require" {
        // 默认匹配的节点的亲和性标签
        if current.Spec.RunAfter.AffinityStratgey.Require == nil {
            current.Spec.RunAfter.AffinityStratgey.Require = []string{
                "kubernetes.io/hostname"
            }
        }
        for _, label := range(current.Spec.RunAfter.AffinityStratgey.Require) {
            // 根据 label name 匹配相关的 affinity
            value := prevOpNodeLabels[label]
            // 转为 NodeSelectorTerm (require)
            return transfromToNodeSelectorTerm(label, value)
        }
    }
    
    // 3. prefer 亲和性继承
    if policy == "prefer" {
        // 默认匹配的节点的亲和性标签
        if current.Spec.RunAfter.AffinityStratgey.Prefer == nil {
            current.Spec.RunAfter.AffinityStratgey.Prefer = []Prefer {
                {
                    Name: "kubernetes.io/hostname",
                    Weight: 100,
                },
            }
        }
        for _, prefer := range(current.Spec.RunAfter.AffinityStratgey.Prefer) {
            // 根据 label name 匹配相关的 affinity
            value := prevOpNodeLabels[prefer.Name]
            // 转为 PreferredSchedulingTerm，并设置 Weight
            return transfromToPreferredSchedulingTerm(prefer.Name, prefer.Weight, value)
        }
    }
}
```



### 示例

示例1：DataFlow: Op_A, Op_B，要求 Op_B 要求跟 Op_A 运行在同一个 Node 中；

```yaml
# Operation B 配置亲和性策略如下
metadata:
	name: B
spec:
	operationRef:
		name: A
		affinityStrategy:
			policy: Require
			# require 也可以不配，默认是 kubernetes.io/hostname
			require: 
			- "kubernetes.io/hostname"
```



示例2：DataFlow: Op_A, Op_B，Op_B 优先跟 Op_A 运行在同一个 region 中；

```yaml
# Operation B 配置亲和性策略如下
metadata:
	name: B
spec:
	operationRef:
		name: A
		affinityStrategy:
			policy: Require
			prefer: 
			- "topology.kubernetes.io/region"
```



示例3：DataFlow: Op_A, Op_B，Op_A 配置特殊的节点亲和性（如 有限运行在 rack-A 的节点上），Op_B 要求跟 Op_A 运行在同一个 rack 中；

```yaml
# Operation A 配置运行在 rack A 的节点上
metadata:
	name: B
spec:
	affinity:
		nodeAffinity：
			preferredDuringSchedulingIgnoredDuringExecution：
				weight: 100,
				preference：
				- matchExpressions：
					- key："label.rack"
					  operator: In
					  values: ["rack-A"]
# Operation B 配置要求跟 A Pod 运行在同一个 rack 中
metadata:
	name: B
spec:
	operationRef:
		name: A
		affinityStrategy:
			policy: Require
			require: 
			- "label.rack"
```



## 相关
DataOperation 的 Pod 不使用 Webhook 进行亲和性注入（Serverful/Serverless）。
